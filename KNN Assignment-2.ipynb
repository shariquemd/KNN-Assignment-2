{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b383ff1-64ab-4b27-a66a-d7cc9cc261fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. \n",
    "The main difference between the Euclidean distance metric and the Manhattan distance metric lies in the way distance is calculated. Euclidean distance is the straight-line distance between two points, while Manhattan distance is the sum of the absolute differences between corresponding coordinates. This affects the sensitivity to the scale of features. Euclidean distance is more sensitive to differences in scale, and this might affect the performance of a KNN classifier or regressor if features have varying scales. In such cases, it's crucial to normalize or standardize the features.\n",
    "\n",
    "Q2. \n",
    "Choosing the optimal value of k involves balancing bias and variance. One common technique is cross-validation, where the dataset is split into training and validation sets multiple times, and the average performance across these splits is used to determine the optimal k. Another method is grid search, where different k values are tested, and the one with the best performance is chosen.\n",
    "\n",
    "Q3. \n",
    "The choice of distance metric can significantly impact KNN performance. Euclidean distance is suitable for continuous, normally distributed data, while Manhattan distance may be more robust to outliers and variations in feature scales. The choice depends on the nature of the data and the problem at hand.\n",
    "\n",
    "Q4.\n",
    "Common hyperparameters in KNN classifiers and regressors include k (number of neighbors), the distance metric, and the weighting scheme for neighbors (uniform or distance-weighted). Tuning these hyperparameters involves experimenting with different values and evaluating performance metrics using techniques like grid search or random search.\n",
    "\n",
    "Q5. \n",
    "The size of the training set affects KNN performance. A smaller training set may lead to overfitting, while a larger one may result in increased computational cost. Optimizing the size involves finding a balance where the model generalizes well without excessive computation.\n",
    "\n",
    "Q6. \n",
    "Drawbacks of KNN include sensitivity to irrelevant features, high computational cost in high-dimensional spaces, and the need for feature scaling. To overcome these, techniques like feature selection, dimensionality reduction, and normalization are employed. Additionally, using algorithms that address these issues, such as k-d trees or ball trees, can improve performance in high-dimensional spaces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
